{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignment 1: Two dimensional Conditional Density Estimation\n",
    "\n",
    "Deadline: Monday, 1/15/2024 9pm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your name: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from math import pi, log\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "from torch import tensor\n",
    "from torch.distributions import Normal\n",
    "import matplotlib as mpl\n",
    "\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load()\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"\n",
    "\n",
    "_ = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# The data generating model\n",
    "Generate data from $x =  \\begin{pmatrix} 2 \\cdot \\sin(\\theta)\\\\  \\cos( \\theta) \\end{pmatrix} + \\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2  \\end{pmatrix}$. \\\n",
    "We get $x \\in R^2$ with two independent noise terms \\\n",
    "$\\epsilon_1 \\sim \\mathcal{N}(0, scale_1)$ \\\n",
    "$\\epsilon_2 \\sim \\mathcal{N}(0, scale_2)$. \n",
    "\n",
    "**Our goal is to learn the conditional distribution $p(x| \\theta, \\mathcal{D})$, for some data $\\mathcal{D}$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulator(theta, noise_scale=[0.05, 0.2]):\n",
    "    \"\"\" \n",
    "    Simulator for the 2D toy example.\n",
    "    args:\n",
    "        theta: torch.Tensor of shape (n, 1)\n",
    "        noise_scale: list of length 2, noise scale for each dimension\n",
    "    returns:\n",
    "        x: torch.Tensor of shape (n, 2)\n",
    "    \"\"\"\n",
    "\n",
    "    # noiseless simulator\n",
    "    x1 = 2 * torch.sin(theta)\n",
    "    x2 = torch.cos(theta)\n",
    "    x = torch.concat([x1.T, x2.T]).T\n",
    "\n",
    "    # create noise\n",
    "    noise = torch.randn(x.shape) * torch.tensor(noise_scale)\n",
    "\n",
    "    return x + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate the training data $\\mathcal{D}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# generate data\n",
    "n = 10_000  # number of datapoints\n",
    "d = 1  # dimensionality of parameters theta\n",
    "\n",
    "# define noise scale\n",
    "noise_scale = [0.4, 0.1]\n",
    "\n",
    "# sample random thetas in [0, 1.5*pi]\n",
    "theta = torch.zeros((n, d))\n",
    "theta[:, 0] = torch.rand(n) * (np.pi * 1.5)\n",
    "\n",
    "samples = simulator(theta, noise_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training data\n",
    "sc = plt.scatter(samples[:, 0], samples[:, 1], c=theta[:, 0], vmin=0, vmax=1.5 * np.pi)\n",
    "\n",
    "# plot an example point\n",
    "example_theta = torch.tensor(np.pi).unsqueeze(0)\n",
    "example_x = simulator(example_theta, noise_scale=[0, 0])\n",
    "\n",
    "plt.plot(\n",
    "    example_x[0],\n",
    "    example_x[1],\n",
    "    \"o\",\n",
    "    color=\"r\",\n",
    "    label=f\"simulator({np.round(example_theta.item(),2)})\",\n",
    ")\n",
    "plt.plot(\n",
    "    [example_x[0] - noise_scale[0], example_x[0] + noise_scale[0]],\n",
    "    [example_x[1], example_x[1]],\n",
    "    color=\"r\",\n",
    ")\n",
    "plt.plot(\n",
    "    [example_x[0], example_x[0]],\n",
    "    [example_x[1] - noise_scale[1], example_x[1] + noise_scale[1]],\n",
    "    color=\"r\",\n",
    "    label=\"+-std\",\n",
    ")\n",
    "\n",
    "\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.colorbar(sc, label=\"$\\\\theta$\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want now to learn a conditional distribution $\\hat{p}(x|\\theta)$ s.t. for every input $\\theta$ we get an approximation of the true (two dimensional) conditional data distribution.\\\n",
    "This homework is divided into 3 exercises:\n",
    "1. implement (the log probability of) a two dimensional Normal distribution (3 points), \n",
    "2. implement a conditional network to learn $p(x|\\theta)$ (5 points),\n",
    "3. evaluate the infered network (2 points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise 1 \n",
    "(3 points)\n",
    "\n",
    "### Learning mean and covariance matrix for a two dimensional Normal distribution conditional on inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. PDF for a two dimensional Normal distribution**\n",
    "\n",
    "Before we define the conditioning network, we need to define a function which evaluates the probability of a two dimensional Normal distribution. \n",
    "For numerical reasons (can you explain why?) we do not evaluate the probability, but directly the log-probability:\n",
    "$$\\log(\\mathcal{N(x, \\mu, \\Sigma )}),$$\n",
    "for $x\\in R^2$, mean $\\mu \\in R^2 $ and covariance matrix $\\Sigma \\in R^{2 \\times 2}$.\n",
    "\n",
    "*Task:*\\\n",
    "Please implement the pdf which also works for batched inputs (e.g. which is able to calculate the pdf for *n* values at the time).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hint:* \\\n",
    "The general formula for the probability density function (pdf) for a k-dimensional Normal distribution is\n",
    "$$ f(x) = \\frac{\\exp(-0.5(x-\\mu)^T\\Sigma^{-1}(x-\\mu))}{\\sqrt{(2\\pi)^k \\det(\\Sigma )}}$$\n",
    "\n",
    "What happens if you take the log of this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_prob_Gauss(x, mean, sigma):\n",
    "    \"\"\"evaluates the log probability for a 2d Normal distribution\n",
    "\n",
    "    Args:\n",
    "        x (tensor): the points to evaluate, shape (batch, 2)\n",
    "        mean (tensor): mean of the distribution, shape (batch, 2)\n",
    "        sigma (tensor): covariance matrix, shape (batch, 2, 2)\n",
    "\n",
    "    Returns:\n",
    "        tensor: log-probabilities p(x|mean,sigma)\n",
    "    \"\"\"\n",
    "\n",
    "    ###\n",
    "    # your code goes here\n",
    "    #\n",
    "    ###\n",
    "\n",
    "\n",
    "\n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation of Solution\n",
    "\n",
    "Pytorch implements Gaussian distributions with a `.log_prob()` method. You can use this to validate your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation of log_prob_Gauss\n",
    "\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "# create random means and cov matrices\n",
    "means = torch.randn((2,))\n",
    "sigma_raw = torch.randn((2, 2))\n",
    "sigmas = sigma_raw @ sigma_raw.transpose(0, 1) + 0.01 * torch.eye(2)\n",
    "\n",
    "# initialize the distribution\n",
    "true_dist = MultivariateNormal(means, sigmas)\n",
    "\n",
    "# sample from the distribution\n",
    "x = true_dist.sample((10,))\n",
    "\n",
    "# calculate the log prob from the pytorch distribution\n",
    "log_prob_pytorch = true_dist.log_prob(x)\n",
    "log_prob_own = get_log_prob_Gauss(x, means, sigmas)\n",
    "\n",
    "# print the logprobs\n",
    "print(log_prob_pytorch, \"\\n\", log_prob_own)\n",
    "# check if they are equal\n",
    "print(\"These log probs are equal: \", torch.allclose(log_prob_own, log_prob_pytorch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "(1 + 4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditioning network\n",
    "\n",
    "\n",
    "As we have seen in the lecture, we can define a neural network (NN), that takes as input the value of $\\theta $ and predicts the parameters for the conditional distribution:\n",
    "$NN(\\theta) = (\\mu, \\Sigma )$.\n",
    "For a one dimensional distribution, the only constraint is $\\sigma >0$. \n",
    "For a two dimensional distribution it gets already a bit more tricky: the covariance matrix $\\Sigma$ needs to be symmetric and positive definite. However, we can express $\\Sigma$ in terms of the marginal variances $\\sigma_1, \\sigma_2 >0$ and the correlation $\\rho \\in [-1,1]$ in the following way:\n",
    "$$\\Sigma = \\begin{bmatrix} \\sigma_1^2 & \\rho \\sigma_1 \\sigma_2 \\\\ \\rho \\sigma_1 \\sigma_2 & \\sigma_2^2  \\end{bmatrix}.$$\n",
    "\n",
    "*Tasks:*\n",
    "\n",
    "a) Implement a neural net, that takes $\\theta$ as input and returns some (how many do you need?) values that are in in b) converted to the parameters of the Normal distribution. The network should be fully connected, and have one hidden layer with a reasonable number of units. Don't forget to include an activation function. \n",
    "\n",
    "b) Implement two functions `get_conditional_params(nn_output)` that converts the output of the neural net two the parameters (mean, var, rho) and  `get_sigma(var, rho)` which returns the corresponding covariance matrix $\\Sigma$.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a)\n",
    "\n",
    "#net = ????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b)\n",
    "\n",
    "\n",
    "def get_conditional_params(nn_output):\n",
    "    \"\"\"converts NN output to mean, var, rho\n",
    "    such that all constraints are fullfilled.\n",
    "\n",
    "    Args:\n",
    "        nn_output (tensor): (batch, ????)\n",
    "\n",
    "    Returns:\n",
    "        tensor: mean, var, rho\n",
    "            with shapes: (batch, 2)(batch, 2)(batch,1)\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE GOES HERE\n",
    "\n",
    "    return mean, var, rho\n",
    "\n",
    "\n",
    "def get_sigma(var, rho):\n",
    "    \"\"\"returns the covariance matrix\n",
    "\n",
    "    Args:\n",
    "        var (tensor): marginal variances. shape (batch, 2)\n",
    "        rho (tensor): correlation. shape (batch, 1)\n",
    "\n",
    "    Returns:\n",
    "        tensor: sigma (batch, 2, 2)\n",
    "    \"\"\"\n",
    "\n",
    "    batch = var.shape[0]\n",
    "    sigma = torch.zeros(batch, 2, 2)\n",
    "\n",
    "    # YOUR CODE GOES HERE\n",
    "    # sigma[:, 0, 0] = ???\n",
    "    # ...\n",
    "\n",
    "\n",
    "    return sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put all this together and see if it worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dataset\n",
    "dataset = data.TensorDataset(theta, samples)\n",
    "train_loader = data.DataLoader(dataset, batch_size=100)\n",
    "\n",
    "# specify the optimizer\n",
    "opt = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# initialize loss to store\n",
    "store_loss = []\n",
    "\n",
    "epochs = 20\n",
    "for e in range(epochs):\n",
    "    for theta_batch, x_batch in train_loader:\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # run the forward model to get the raw NN output\n",
    "        nn_output = net(theta_batch)\n",
    "\n",
    "        # convert the output to the corresponding parameters for a Normal distribution\n",
    "        mean, var, rho = get_conditional_params(nn_output)\n",
    "        sigma = get_sigma(var, rho)\n",
    "\n",
    "        # evaluate the Normal distribution\n",
    "        log_prob_Gauss = get_log_prob_Gauss(x_batch, mean, sigma)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = -(log_prob_Gauss).sum()\n",
    "\n",
    "        # take a gradient step\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # store the loss\n",
    "        store_loss.append(loss.detach().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what the loss looks like\n",
    "plt.plot(np.linspace(0, epochs, len(store_loss)), store_loss)\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We now have a neural network which outputs the parameters of a probability density distribution given inputs.\n",
    "In this case, it is a two dimensiona Normal distribution of which we learned the mean and covariance matrix. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inspect the distribution for test points\n",
    "Let's inspect the learned distribution for some test points *theta_test*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "theta_test = torch.tensor([0, torch.pi / 2, torch.pi, torch.pi * 1.5]).unsqueeze(1)\n",
    "nn_output = net(theta_test).detach()\n",
    "conditional_mean, var, rho = get_conditional_params(nn_output)\n",
    "conditional_sigma = get_sigma(var, rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"conditional sigmas:\",\n",
    "    conditional_sigma,\n",
    "    \", \\nconditional var: \",\n",
    "    conditional_sigma[:, 0, 0] ** 0.5,\n",
    "    conditional_sigma[:, 1, 1] ** 0.5,\n",
    ")\n",
    "print(f\"The variance should be neare the used the noise scale of {noise_scale}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also compare the infered mean to the noiseless simulator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get x_test with no noise\n",
    "x_test_noiseless = simulator(theta_test, noise_scale=[0, 0])\n",
    "\n",
    "print(\n",
    "    \"Predicted conditional mean:\\n \",\n",
    "    conditional_mean,\n",
    "    \", \\n noiseless_x:\\n\",\n",
    "    x_test_noiseless,\n",
    ")\n",
    "\n",
    "mse = ((conditional_mean - x_test_noiseless) ** 2).mean()\n",
    "\n",
    "print(\"MSE on test points:\", mse.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Distribution\n",
    "\n",
    "It's a bit hard to evaluate directly from looking at the learned distribution parameters. Let's visualize the learned distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_2d_gaussian(\n",
    "    mean, sigma, true_x, x_range=[-2, 2], y_range=[-2, 2], resolution=100\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize a 2D distribution by evaluating the log probabilities on a grid.\n",
    "\n",
    "    Args:\n",
    "        x_range (tuple): Range of x values (e.g., (-1, 1)).\n",
    "        y_range (tuple): Range of y values (e.g., (-1, 1)).\n",
    "        resolution (int): Number of points in each dimension of the grid.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    x = torch.linspace(x_range[0], x_range[1], resolution)\n",
    "    y = torch.linspace(y_range[0], y_range[1], resolution)\n",
    "    X, Y = torch.meshgrid(x, y)\n",
    "\n",
    "    # evaluate the log probabilities on the grid\n",
    "    X_flat = X.flatten()\n",
    "    Y_flat = Y.flatten()\n",
    "    grid = torch.stack([X_flat, Y_flat], dim=1)\n",
    "    log_probs = get_log_prob_Gauss(grid, mean, sigma).view_as(X)\n",
    "\n",
    "    plt.contourf(\n",
    "        X.numpy(), Y.numpy(), torch.exp(log_probs).numpy(), levels=20, cmap=\"viridis\"\n",
    "    )\n",
    "    if true_x is not None:\n",
    "        plt.plot(true_x[0], true_x[1], \"o\", color=\"r\", label=\"true mean\")\n",
    "        plt.legend()\n",
    "    plt.colorbar(label=\"Probability\")\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(x_test_noiseless[0].size())\n",
    "visualize_2d_gaussian(\n",
    "    conditional_mean[0], conditional_sigma[0], true_x=x_test_noiseless[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "(2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Alternative (visual) evaluation: for every $\\theta$, sample from the predicted Gaussian distribution\n",
    "\n",
    "We can visually compare the learned conditioned distributions by comparing samples from these distributions with our data samples.\n",
    "\n",
    "*Task:* \\\n",
    "Complete the code below to sample points $x\\sim p(\\cdot |\\theta,\\mathcal{D})$ from the inferred conditional distribution for the first 500 points of $\\theta$.\n",
    "\n",
    "*Remarks:*\n",
    "- You can use the pytorch implementation  to sample from a multivariate Normal distribution (`torch.distributions.multivariate_normal.MultivariateNormal`)\n",
    "\n",
    "- You can compare these samples with the training data in the plotting cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the thetas to test on\n",
    "n = 500\n",
    "theta_test = theta[:n]\n",
    "\n",
    "# predict mean and sigma for theta_test\n",
    "\n",
    "#####\n",
    "# YOUR CODE \n",
    "# ###\n",
    "\n",
    "# initialize MultivariateNormal and sample\n",
    "\n",
    "#####\n",
    "# YOUR CODE \n",
    "#\n",
    "# samples_test = ...\n",
    "# ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(samples[:500, 0], samples[:500, 1], c=\"grey\", label=\"data\")\n",
    "sc = plt.scatter(\n",
    "    samples_test[:, 0],\n",
    "    samples_test[:, 1],\n",
    "    c=theta_test[:, 0],\n",
    "    vmin=0,\n",
    "    vmax=1.5 * np.pi,\n",
    "    label=\"predicted samples\",\n",
    ")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.colorbar(sc, label=\"$\\\\theta$\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! You managed the first assignment!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "sbi-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Short primer on pytorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from math import pi, log\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "_ = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A very brief intro to PyTorch\n",
    "\n",
    "Say we are given some data $x$ that is generated by some process $f(\\theta) + noise$. \\\n",
    "It looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "n = 200  # number of datapoints\n",
    "d = 1  # dimensionality of parameter space theta\n",
    "\n",
    "theta = torch.rand((n, d))  # uniform\n",
    "\n",
    "\n",
    "def func(theta):\n",
    "    return theta + 0.3 * torch.sin(2 * pi * theta)\n",
    "\n",
    "\n",
    "noise = torch.randn((n, d)) * 0.05\n",
    "x = func(theta) + noise\n",
    "\n",
    "# Data t_train and x_train\n",
    "with mpl.rc_context(fname=\".matplotlibrc\"):\n",
    "    fig = plt.figure(figsize=(4.5, 2.2))\n",
    "    plt.plot(theta[:400], x[:400], \"go\", alpha=0.4, markerfacecolor=\"none\")\n",
    "    plt.xlabel(r\"$\\theta$\")\n",
    "    plt.ylabel(\"x\")\n",
    "    plt.savefig(\"figures/data_samples.png\", dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"figures/data_samples.png\" alt=\"drawing\" width=\"900\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We want to train a neural network to predict $x$ from $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First, let's define our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dataset = data.TensorDataset(theta, x)\n",
    "train_loader = data.DataLoader(dataset, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then, we build a neural network. The weights of this network will be optimized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(1, 20), nn.ReLU(), nn.Linear(20, 20), nn.ReLU(), nn.Linear(20, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what is our \"loss\" function? \\\n",
    "As we want to learn the function $f$ the model should learn the mean value of $f(θ)$ for all $θ$. \n",
    "\n",
    "So we define the loss to be the mean-squared error (MSE):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(prediction, data):\n",
    "    return torch.mean((prediction - data) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We define an optimizer which optimizes the weights (e.g. with gradient descent):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "opt = optim.Adam(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Finally, we build a training loop where we optimize the weights with a Mean-Squared error loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "training_loss = []\n",
    "n_epochs = 200\n",
    "for e in range(n_epochs):\n",
    "    for theta_batch, x_batch in train_loader:\n",
    "        opt.zero_grad()\n",
    "        nn_output = net(theta_batch)\n",
    "        loss = get_loss(nn_output, x_batch)\n",
    "        training_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(training_loss)) / len(train_loader), training_loss)\n",
    "plt.xlabel(\"training epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's see if the neural network learned something..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for t in torch.linspace(0, 1, 100):\n",
    "    predictions.append(net(torch.as_tensor([t])))\n",
    "predictions = torch.stack(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "with mpl.rc_context(fname=\".matplotlibrc\"):\n",
    "    fig = plt.figure(figsize=(4.5, 2.2))\n",
    "    plt.plot(theta[:400], x[:400], \"go\", alpha=0.4, markerfacecolor=\"none\")\n",
    "    plt.xlabel(r\"$\\theta$\")\n",
    "    plt.ylabel(\"x\")\n",
    "    plt.plot(np.linspace(0, 1, 100), predictions.detach().numpy())\n",
    "    plt.savefig(\"figures/nn_prediction.png\", dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"figures/nn_prediction.png\" alt=\"drawing\" width=\"900\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main ingredients for Deep Learning\n",
    "You saw the main ingredients of most deep-learning pipelines:\n",
    "1. A dataset\n",
    "2. The actual network\n",
    "3. A loss function\n",
    "4. An optimizer\n",
    "5. The actual training loop"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "base_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
